{"metadata":{"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q -U transformers datasets bitsandbytes peft","metadata":{"id":"fxnhvi_vEqy6","execution":{"iopub.status.busy":"2024-09-21T13:21:00.692354Z","iopub.execute_input":"2024-09-21T13:21:00.693298Z","iopub.status.idle":"2024-09-21T13:21:32.622245Z","shell.execute_reply.started":"2024-09-21T13:21:00.693248Z","shell.execute_reply":"2024-09-21T13:21:32.621132Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\nHF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")","metadata":{"execution":{"iopub.status.busy":"2024-09-21T13:21:32.624339Z","iopub.execute_input":"2024-09-21T13:21:32.624660Z","iopub.status.idle":"2024-09-21T13:21:32.825035Z","shell.execute_reply.started":"2024-09-21T13:21:32.624626Z","shell.execute_reply":"2024-09-21T13:21:32.824275Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import huggingface_hub\n\nhuggingface_hub.login(token=HF_TOKEN)","metadata":{"execution":{"iopub.status.busy":"2024-09-21T13:21:32.826200Z","iopub.execute_input":"2024-09-21T13:21:32.826599Z","iopub.status.idle":"2024-09-21T13:21:33.355967Z","shell.execute_reply.started":"2024-09-21T13:21:32.826551Z","shell.execute_reply":"2024-09-21T13:21:33.355100Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_dataset\ndataset = load_dataset(\"BLACKBUN/old_korean_newspaper_1897_1910_economy_politic_qa\")","metadata":{"id":"9XpaNb-5Eqy9","execution":{"iopub.status.busy":"2024-09-21T13:21:33.358089Z","iopub.execute_input":"2024-09-21T13:21:33.358477Z","iopub.status.idle":"2024-09-21T13:21:37.070487Z","shell.execute_reply.started":"2024-09-21T13:21:33.358432Z","shell.execute_reply":"2024-09-21T13:21:37.069505Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/410 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e717b43d7694d01a93ee7bb6250f831"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/3.46M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8944082078e3411c8497b90b8192811e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/112k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"163744f77bce42a3bbde2b4c6692dc6d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/8946 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16e424c6fe064abb9c05294184f93eb7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/284 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4eddf592a9e240d28d82da4f7ec59949"}},"metadata":{}}]},{"cell_type":"code","source":"# for 2 billion model\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\nfrom peft import PeftModel\nimport torch\n\nBASE_MODEL = \"google/gemma-2-2b-it\"\nADAPTER = \"jia6776/korean_history_1897_1910_gemma2_2b_lora\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    BASE_MODEL,\n    device_map=\"auto\"  # This will automatically distribute the model across available GPUs\n)\n\n# Load the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n\n# Load the LoRA adapter\nmodel = PeftModel.from_pretrained(model, ADAPTER)","metadata":{"id":"_HGQKGPVE_v4","execution":{"iopub.status.busy":"2024-09-21T13:21:37.071832Z","iopub.execute_input":"2024-09-21T13:21:37.072369Z","iopub.status.idle":"2024-09-21T13:22:28.915101Z","shell.execute_reply.started":"2024-09-21T13:21:37.072329Z","shell.execute_reply":"2024-09-21T13:22:28.913922Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8ab3db8c5c44013be223472b8584183"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/838 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44b84e2a2af747428155d68cf1c199f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/24.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"607bdfb513af422998d06fd12721d9ea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"063b732a6a7149aeaa7790b6c1542334"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"354f5a095ff34d2d99c276887d1eb7e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/241M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4edc22ce2eb24948a84ea94c87707789"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38a337a8c72b41e2886657cfa51179b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b585823b7264d70bcbacabb47c597b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/47.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33deef5a789a4ee8a57a7b5573177262"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"912599d3b1df4ac99d1627928bcd7e2c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ec89ee256854d1088b777447963b9fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b41bb98718fc4379adfe666d5cd52f08"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_config.json:   0%|          | 0.00/720 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5585a8e31d154c0bb4420acfcc8930db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/31.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ac4bb920ddc45b086d9707a8e242cfa"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom peft import PeftModel\nimport torch\n\nBASE_MODEL = \"google/gemma-2-9b-it\"\nADAPTER = \"BLACKBUN/korean_history_1897_1910_gemma2_9b_lora_q4\"\n\n# Configure 4-bit quantization\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\n# Load the model with 4-bit quantization\nmodel = AutoModelForCausalLM.from_pretrained(\n    BASE_MODEL,\n    quantization_config=bnb_config,\n    device_map=\"auto\"  # This will automatically distribute the model across available GPUs\n)\n\n# Load the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n\n# Load the LoRA adapter\nmodel = PeftModel.from_pretrained(model, ADAPTER)","metadata":{"execution":{"iopub.status.busy":"2024-09-21T12:08:22.430871Z","iopub.execute_input":"2024-09-21T12:08:22.431370Z","iopub.status.idle":"2024-09-21T12:11:15.243857Z","shell.execute_reply.started":"2024-09-21T12:08:22.431328Z","shell.execute_reply":"2024-09-21T12:11:15.242982Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa8a480e9f9b4ea3acc1cd5dedb1227c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/857 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c77736ae32324ef18aa20480ac22ef5d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/39.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b86b66d96f324a818e993468e8ded3a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e1bacaa16e844cd9e845be344bc38d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f7f822bf8d5438b834bed31da51965e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0d0acc35991400fb2aa309939172c60"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"299aab44a2884c5e906fce1546a1ebdb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/3.67G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26b88cad8cf74e7391a35e521a4b657f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ec45a77cc2b4fe39ee71512c9e86422"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/173 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d09b26d762a847f985b471cbe13888b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/47.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"142bfe3934c041f180d4b5a5a32b9d1f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26740ef7da734686a083f656f9ab9931"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8a3a71d937a49979895a39b43b05012"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"675228f7d0ec4561a990e5842f0f4343"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_config.json:   0%|          | 0.00/720 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a41a6e733a24456d9e6a76836129a641"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/81.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0bf669af995489586dbe7e0d09a3365"}},"metadata":{}}]},{"cell_type":"code","source":"question = dataset['test']['Question'][149]\nanswer = dataset['test']['Answer'][149]","metadata":{"execution":{"iopub.status.busy":"2024-09-21T13:27:11.352556Z","iopub.execute_input":"2024-09-21T13:27:11.353306Z","iopub.status.idle":"2024-09-21T13:27:11.359590Z","shell.execute_reply.started":"2024-09-21T13:27:11.353267Z","shell.execute_reply":"2024-09-21T13:27:11.358576Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"print(f\"Q:{question}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-21T13:27:11.652329Z","iopub.execute_input":"2024-09-21T13:27:11.652931Z","iopub.status.idle":"2024-09-21T13:27:11.657906Z","shell.execute_reply.started":"2024-09-21T13:27:11.652895Z","shell.execute_reply":"2024-09-21T13:27:11.656962Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Q:대한제국 시기의 관보에서 언급된 '의정부'와 '궁내부'의 역할은 무엇인가요?\n","output_type":"stream"}]},{"cell_type":"code","source":"messages = [\n    # {\n    #     \"role\": \"system\",\n    #     \"content\": \"You are a helpful assistant.\"\n    # },\n    {\n        \"role\": \"user\",\n        \"content\": f\"{question}\"\n    },\n\n]\nprompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)","metadata":{"execution":{"iopub.status.busy":"2024-09-21T13:27:20.847364Z","iopub.execute_input":"2024-09-21T13:27:20.847990Z","iopub.status.idle":"2024-09-21T13:27:20.852945Z","shell.execute_reply.started":"2024-09-21T13:27:20.847946Z","shell.execute_reply":"2024-09-21T13:27:20.851987Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"input_text = prompt\ninputs = tokenizer(input_text, return_tensors=\"pt\").to('cuda')\noutputs = model.generate(**inputs, max_length=256)\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)","metadata":{"execution":{"iopub.status.busy":"2024-09-21T13:27:20.941801Z","iopub.execute_input":"2024-09-21T13:27:20.942388Z","iopub.status.idle":"2024-09-21T13:27:31.120397Z","shell.execute_reply.started":"2024-09-21T13:27:20.942350Z","shell.execute_reply":"2024-09-21T13:27:31.119541Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"print(f\"Model Answer: {generated_text}\\n\")\nprint(f\"Ground Answer: {answer}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-21T13:27:31.121914Z","iopub.execute_input":"2024-09-21T13:27:31.122211Z","iopub.status.idle":"2024-09-21T13:27:31.127624Z","shell.execute_reply.started":"2024-09-21T13:27:31.122180Z","shell.execute_reply":"2024-09-21T13:27:31.126430Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Model Answer: user\n대한제국 시기의 관보에서 언급된 '의정부'와 '궁내부'의 역할은 무엇인가요?\nmodel\n'의정부'는 정부의 정책을 수행하고, '궁내부'는 황실과 관련된 행정을 담당하는 부서로, 두 부서 모두 정부의 중앙에서 중요한 역할을 수행했습니다. 의정부는 정부의 정책을 논의하고, 궁내부는 황실의 행정을 감독하며, 두 부서가 협력하여 국가의 운영을 보조하는 역할을 했습니다.\n\nGround Answer: 대한제국 시기의 관보에서 '의정부'는 국가의 정치적 의사결정을 담당하는 기관으로, 정부의 정책과 법률을 논의하고 결정하는 역할을 했습니다. '궁내부'는 황실과 관련된 업무를 수행하며, 궁중의 행정과 의전 등을 관리하는 기관으로, 황제의 권위를 지키고 궁중의 질서를 유지하는 역할을 했습니다. 이 두 기관은 서로 협력하여 국가 운영에 중요한 역할을 하였으며, 관보의 내용은 이러한 기관들이 어떻게 상호작용하며 국가의 중요한 사안을 처리했는지를 보여줍니다.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}