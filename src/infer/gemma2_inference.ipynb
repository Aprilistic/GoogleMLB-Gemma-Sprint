{"metadata":{"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q -U transformers datasets bitsandbytes peft","metadata":{"id":"fxnhvi_vEqy6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\nHF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")","metadata":{"execution":{"iopub.status.busy":"2024-09-21T12:05:54.924005Z","iopub.execute_input":"2024-09-21T12:05:54.924455Z","iopub.status.idle":"2024-09-21T12:05:55.088404Z","shell.execute_reply.started":"2024-09-21T12:05:54.924408Z","shell.execute_reply":"2024-09-21T12:05:55.087273Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import huggingface_hub\n\nhuggingface_hub.login(token=HF_TOKEN)","metadata":{"execution":{"iopub.status.busy":"2024-09-21T12:05:55.090273Z","iopub.execute_input":"2024-09-21T12:05:55.090622Z","iopub.status.idle":"2024-09-21T12:05:55.728929Z","shell.execute_reply.started":"2024-09-21T12:05:55.090582Z","shell.execute_reply":"2024-09-21T12:05:55.727827Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_dataset\ndataset = load_dataset(\"BLACKBUN/old_korean_newspaper_1897_1910_economy_politic_qa\")","metadata":{"id":"9XpaNb-5Eqy9","execution":{"iopub.status.busy":"2024-09-21T12:05:55.730791Z","iopub.execute_input":"2024-09-21T12:05:55.731191Z","iopub.status.idle":"2024-09-21T12:05:59.803641Z","shell.execute_reply.started":"2024-09-21T12:05:55.731143Z","shell.execute_reply":"2024-09-21T12:05:59.802377Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/410 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8c5ba2638574fa6984ef9058f140ee2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/3.46M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8bdf9801acda4c63827116d868349161"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/112k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1118d1edf6fb419381f63d5ddd765193"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/8946 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3d0d0b3b90148fdbc63995fc719ef55"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/284 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"448bdbf810ac43b3a248a3b75e6ba568"}},"metadata":{}}]},{"cell_type":"code","source":"# for 2 billion model\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\nfrom peft import PeftModel\nimport torch\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nBASE_MODEL = \"google/gemma-2-2b-it\"\nADAPTER = \"jia6776/korean_history_1897_1910_gemma2_2b_lora\"\n\nmodel = AutoModelForCausalLM.from_pretrained(BASE_MODEL, device_map=\"auto\")\ntokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n\nmodel = PeftModel.from_pretrained(model, ADAPTER).to(device)\n","metadata":{"id":"_HGQKGPVE_v4","execution":{"iopub.status.busy":"2024-09-21T12:05:59.805319Z","iopub.execute_input":"2024-09-21T12:05:59.806011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom peft import PeftModel\nimport torch\n\nBASE_MODEL = \"google/gemma-2-9b-it\"\nADAPTER = \"BLACKBUN/korean_history_1897_1910_gemma2_9b_lora_q4\"\n\n# Configure 4-bit quantization\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\n# Load the model with 4-bit quantization\nmodel = AutoModelForCausalLM.from_pretrained(\n    BASE_MODEL,\n    quantization_config=bnb_config,\n    device_map=\"auto\"  # This will automatically distribute the model across available GPUs\n)\n\n# Load the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n\n# Load the LoRA adapter\nmodel = PeftModel.from_pretrained(model, ADAPTER)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"question = dataset['test']['Question'][10]\nanswer = dataset['test']['Answer'][10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Q:{question}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"messages = [\n    # {\n    #     \"role\": \"system\",\n    #     \"content\": \"You are a helpful assistant.\"\n    # },\n    {\n        \"role\": \"user\",\n        \"content\": f\"{question}\"\n    },\n\n]\nprompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_text = prompt\ninputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\noutputs = model.generate(**inputs, max_length=256)\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Model Answer: {generated_text}\\n\")\nprint(f\"Ground Answer: {answer}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipe_finetuned = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=512, device=\"cuda\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}